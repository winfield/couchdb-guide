## 集群 ##

好了, 你已经看到本书的这里了. 我相信你或多或少已经理解什么是CouchDB以及它的API是怎么用的了. 或许你已经部署了一两个应用, 而且已经需要处理相当多的流量, 要开始考虑扩展的问题了. "扩展"是一个很不精确的词, 但是在本章节中, 我们将要具体处理的问题是, 如何创建一个分区的或者分片的集群. 这个集群在上线后可以以某个速率增长.

我们将会来观察一个拥有稳定节点的CouchDB集群如何进行请求与响应的分发. 然后我们会讲解如何来增加多余的热备节点, 这样就不用担心因某些机器崩溃而导致问题发生了. 在一个大型的集群中, 应该准备好会有5-10%的机器可能会崩溃或者经历性能低下, 所以集群的设计必须要防止某些节点的崩溃会影响到可靠性. 最后, 我们会来看看如何通过使用复制来分割或者合并节点, 从而动态的进行集群布局的调整.

### 介绍CouchDB Lounge ###

CouchDB Lounge是一个构建于代理之上的, 用于分区和建立集群的应用, 它最初是由Meebo开发出来的, Meebo是一个基于web的即时消息服务. Lounge有两个主要的组件: 一个用来处理简单的文档GET和PUT请求, 而另一个则用于处理视图的请求.

dubmproxy会处理任何非CouchDB视图请求的简单请求. 它作为nginx的一个模块存在, nginx是一个高性能的反向HTTP代理. 得益于反向HTTP代理的工作方式, 这使得很多东西都变得可配置化, 比如安全选项, 加密, 负载分发, 压缩, 当然还有数据库资源的大量缓存.

smartproxy则只会处理CouchDB视图请求, 并且会把它们分发到集群中的所有节点, 使集群的视图处理变得更加高效. 它作为Twisted的一个守护进程存在, Twisted是一个用Python写的高性能的事件驱动型网络编程框架.

### 一致性哈希处理 ###

CouchDB的存储模型使用唯一的ID来保存和读取文档. Lounge的核心有一个简单的方法来对文档ID进行哈希处理. 接着, Lounge会使用哈希的前几个字符来决定请求要分发到哪里去. 你可以通过编写一个共享映射(shared map)来配置其行为, 它只是一个简单的文本配置文件.

因为Lounge会分配给每个节点一部分的哈希(这一部分的哈希被称为密钥空间), 所以你可以加入任意数量的节点. 因为哈希函数产生的16进制字符串和文档ID完全没有关系, 而我们又是通过这些哈希的前几个字符来进行的请求分发, 所以可以保证所有的节点都会有相同的负载. 而且, 哈希函数拥有一致性, Lounge从HTTP请求URI中得到任意的文档ID都会生成同样的哈希, 所以每次都会指向同一个节点.

这种基于密钥空间进行分片的思想通常可以被图解为一个圆环. 每一个tic标记标明了两个分区密钥空间之间的边界. 哈希函数则把文档ID映射到圆环上的各个位置. 因为圆环是连续的, 所以总是可以通过把一个分区再分成多个分区, 来加入更多的节点. 比如有四个物理服务器, 那么就可以通过下面这样的分配方式, 把它们分配为16个独立的密钥空间.

A	0,1,2,3
B	4,5,6,7
C	8,9,a,b
D	c,d,e,f

如果文档ID的哈希以0开头, 那么它就会被分发到分区A. 类似的, 以1, 2, 3开头的也是如此. 然而, 如果哈希以c, d, e或者f开头, 那么它就会被分发到分区D. 作为一个完整的例子, 哈希71db329b58378c8fa8876f0ec04c72e5会被分配到上面表格中节点B的数据库7. 在后端集群中, 你可以把它分配到http://B.couches.local/db-7/类似的地址. 通过这种方式, 哈希表就只是一个哈希到后端数据库URI的映射关系了. 如果这些听起来还是很复杂, 别急; 你所要做的只是提供一个分区到节点的映射, Lounge会合理创建哈希圈--所以如果你不想干, 就没有必要去干这些脏活了.

如果要在web架构上实现相同的概念, 代理服务器可以根据请求URL来对文档进行分区, 而不需要查看请求体, 因为CouchDB使用HTTP协议. 这个REST架构背后的一个核心原则, 也是HTTP协议提供给我们的众多好处中的一个. 在实际中, Lounge会对请求URI进行哈希处理, 再将其比对结果, 最后找出它所属的那一部分密钥空间. 然后, Lounge会在配置表格中查找这个哈希的关联分区, 再把HTTP请求转发到后端的CouchDB服务器

一致性哈希处理是一个简单的办法, 可以用来保证你总是可以找到保存的文档, 即使是在跨分区进行存储负载均衡的情况下. 因为哈希函数很简单(它以CRC32为基础), 所以你可以实现自己的HTTP中间件或者客户端, 它们可以类似的把请求分发到数据所在的正确物理位置.

#### 冗余的存储 ####

一致性哈希处理解决了如何把一个单一逻辑数据库保存于一组分区的问题, 这些分区可以分布于多个服务器. 但它不讨论如何在硬件或软件故障时保证数据安全性的问题. 如果你很关心数据的安全性, 除非有至少两份的相同数据拷贝, 最好还是在不同的地理位置, 否则你是不会安心的.

CouchDB的复制功能使得维护热备冗余的slaves或者multi-master形式的负载均衡变得相对的不那么痛苦. 关于如何管理复制的细节在第16章, 复制中有详细讲解. 在这里, 重点是要理解, 维护冗余备份和保证某个特定文档ID总是可以在集群的同一个分区上找到, 这两者是完全不相关的.

为了数据安全, 你可以会想要拥有至少两到三份的所有数据的拷贝. 然而, 如果你封装了冗余, 集群的更上层可以把每一个分区都当成是一个单一的单元, 而让里面的逻辑分区自己去管理冗余和故障.

#### 冗余的代理 ####

就像我们因为不能接受硬件故障可能导致的数据丢失, 我们同样需要运行多个代理节点的实例, 来避免因为某个代理节点崩溃而导致集群的部分不可用. 通过运行冗余的代理实例, 并且在它们之间作负载均衡, 我们可以增加集群的处理能力和可靠性.

#### 视图合并 ####

一致性哈希可以使得文档处于合适的节点, 但文档仍然可以用emit()函数产生任意的key. 增量MapReduce的关键在于把这种能力带到数据层面, 所以我们不应该重复分配产生的key; 而是应该通过HTTP代理来发送查询到CouchDB节点, 然后再使用Twisted Python Smartproxy合并结果.

Smartproxy会发送视图请求到每一个节点, 所以它需要在返回给客户端前把响应数据进行合并. 谢天谢地, 这个操作不是一个很耗资源的操作, 因为不管有多少记录返回, 合并都可以在固定内存空间里完成. Smartproxy会接收每个节点的第一条记录并比较它们. 我们使用CouchDB的校对规则, 根据节点返回记录的key来对节点进行排序. Smartproxy会取出序列中第一个节点里的最上面一条记录, 返回给客户端.

这个过程可以重复进行, 只要客户端还继续要求更多的记录, 但是如果客户端加上了一个限制条件, 那么Smartproxy就必须提早终止响应, 扔掉任何多余的节点记录.

这种布局简单而松散. 优势在于其简单性, 这有利于理解拓扑结构及故障判断. 我们正在把这个工作转到Erlang上, 这样就可以管理动态的集群, 并且可以把集群控制集成到CouchDB的运行时.

#### 集群增长 ####

在web扩展方面使用CouchDB, 很可能会需要对CouchDB集群进行动态的扩展. 做大网站必须连续的增加存储能力, 所有我们需要一个策略, 在不下线网站的情况下, 增加集群的大小. 而有些操作也可能则导致数据量的临时增大, 这种情况下, 我们还需要一个过程来缩减集群而不中断服务.

在这一部分里, 我们会看到如何使用CouchDB的复制过滤器来把一个数据库分割到几个分区, 以及如何使用这项技术来在不下线的情况下增长集群. 你可以使用几个简单的步骤来避免增长集群时的数据库分区.

预先分片(Oversharding)这项技术的意思是: 对集群进行分区, 使得在每个物理节点上都有多个分区. 把一个分区从一台机器移动到另一台, 要比把它分割为更小的分区来得简单, 因为代理服务器所使用的集群映射配置只需要改变指向的分区的位置就行了, 而不需要增加新的逻辑分区. 移动分区也比分割成更小的多个分区占用更少的资源.

有一个需要回答的问题是, "我们应该预先分多少片呢?" 答案取决于你的应用程序和部署方式, but there are some forces that push us in one direction over another. 如果我们找出了正确的所需分区数量, 就会得到一个最优化的可增长集群.

在"视图合并"一节里, 我们讲到, 不管返回有多少记录, 合并总是会在一个固定空间里完全. 然而, 用于合并视图的内存空间和网络资源, 以及文档ID到分区的映射, 的的确确是随着某个特定代理下的分区数量而线性增长的. 因为这个原因, 我们会想要限制每个代理下的分区数量. 然而, 如果集群大小存在一个上限, 这又是我们所不能接受的. 解决方法是使用一个代理树, 根代理分区到一些中间代理, 这些中间代理再映射到数据库节点.

决定每个代理需要管理多少个分区的因素有: 每个独立服务器节点的存储大小, 项目的数据增长速率, 代理可用的网络和内存资源, 以及集群可接受的延时.

保守的假设每个代理有64个分区, 每个节点有1TB的数据存储空间(如果包括压缩所需要占用的空间, 这些节点大约会需要2TB的空间). 那么我们可以发现, 在需要增加分区数量之前, 一个处于这些CouchDB数据节点之前的代理, 能够存储最大64TB的数据(在128个或者也许是192个服务器节点上, 根据系统的冗余要求决定).

通过把数据库节点换成另外一个代理服务器, 并且把这64个分区每个再分区为64个分区, 我们就得到了一个有4096个分区, 深度为2的树. 就像系统初始时那样, 64个分区只需要几个物理节点, 要转化一棵2层的树, 同样不需要使用成千上万的机器. 如果我们假设代理必须运行在它自己独立的节点上, 并且数据库节点可以存放16个分区, 那么可以看到, 我们需要65代理节点和256数据库节点(这里不包含冗余的因素, 如果考虑进去, 就需要在原基础上乘以2或者3了). 要开始构建一个可以平稳的从64TB增长到4PB的集群, 我们一开始可以使用大约600到1000个服务器节点, 当数据量增大时再加入新的节点并把分区移动到那里.

我们已经看过了, 只是一个深度为2的集群就可以存储巨大量的数据了. 简单的计算就可以告诉我们, 通过相同的方法创建一个3层代理的集群, 我们就能够在成千上万台机器上管理262拍的数据. 保守的估计每层会产生的延时是100ms, 这样在没有做性能优化的情况下, 即便是在一个3层的树里, 总体上我们还能拥有300ms的响应时间. 而且我们还是应该能够在小于1秒的时间内, 在巨大的数据集里进行查询.

通过使用预先分片(oversharding)和迭代的把完整分区(只有一个分区的那些数据库节点)替换为指向另一组oversharding分区的代理节点, 我们可以在保持最小延时的同时, 把集群增长到一个非常大的规模.

现在我们需要来看看那两个使得集群可以增长的机制了: 把一个分区从一个过于拥挤的节点移动到一个空节点, 以及把一个大分区分割成为许多小的子分区. 移动分区更简单些, 这也为什么如果需要移动分区就应该进行移动的原因. 而运行更加消耗资源的再分区进程, 只应该在分区过于庞大, 以至于每个数据库服务器只能容纳1到2个分区的时候进行.

#### 移动分区 ####

就像我们先前提到的那样, 每个分区是由N个冗余CouchDB数据库组成的, 每个CouchDB数据库存储在不同的物理服务器上. 为了更加概念化, 假设任何操作都应该会被自动的作用于所有的冗余备份. 为了讨论方便, 我们只讲抽象的分区, 但你应该了解冗余节点拥有同样的大小, 所以在分区进行增长时需要同样的操作.

把一个分区从一个节点移动到另一个的最简单的方法是, 在目标节点上创建一个空的数据库, 然后使用CouchDB的复制功能把老节点数据复制到新节点. 当新分区和原分区一样新后, 代理节点可以重新配置, 指向到新机器. 代理节点指向新分区后, 再进行最后一次的复制, 确保新分区和原分区数据相同, 之后, 老的分区就可以退休了, 可以把原来那台机器上的空间释放掉.

另一个移动分区数据库的方法是通过rsync, 把文件从老节点备份到新的上面. 根据分区最近的压缩情况, 这样做总体上可以更加高效, 使用更低的CPU消耗来初始化一个新节点. 接着, 就可以再使用复制来把rsync同步过来的文件进行更新. 关于rsync和复制的更多信息, 请查看看16章, 复制.

#### 分割分区 ####

运行一个CouchDB集群需要做最后一件事是, 分割过于庞大分区. 在第16章, 复制里, 我们讨论了如何使用_changes API来做连续复制. _changes API可以使用过滤器(请查看第20章, 变更通知). 复制可以配置一个过滤器函数, 来复制一个数据库中总数据的一个子数据集. 分割分区是通过创建目标分区, 然后为它们配置其所需要的哈希值范围来完成的. 然后, 它们运行经过过滤的复制, 从源分区中只复制那些满足它们要求的数据. 这样做的结果就是, 一个源数据有了多份部分拷贝, 每个新分区都得到了同样大小的一份数据. 它们完全组成起来, 就是原数据的一个完整拷贝. 当复制完成, 新分区也完成了冗余备份后, 那些在这些新分区前端的代理就可以上线了, 而最高层的代理则指向这个新代理, 替代掉原来的分区. 就像移动一个分区时所做的那样, 我们应该在集群不再需要老分区后再进行一次复制, 防止有潜在的更新丢失. 当这个复制完成后, 老的分区就可以退休了, 而它的硬件则可以被用于集群的其他地方.
